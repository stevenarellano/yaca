# EffiBench: Benchmarking the Efficiency of Automatically Generated Code

## About

This README contains general information about EffiBench and how to recreate the experiments I've performed. This repository was originally forked from [EffiBench](https://github.com/huangd1999/EffiBench). You can find the original README at [old_README.md](old_README.md).

## Abstract

Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green computing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents Effibench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 code are \textbf{13.89} and \textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on \url{ https://github.com/huangd1999/EffiBench }. We also provide the LeaderBoard in \url{ https://huggingface.co/spaces/EffiBench/effibench-leaderboard }.

## Repository

This repository contains the following directories:

`data/`: Contains the data used in the experiments.

`prompts/`: Contains prompts used to generate code.

`src/`: Contains the source code for the experiments ran, such as the code generation, code running, and metrics calculation.

`src/results/`: Contains the results of each code generation test for each model.

`src/dat_results/`: Contains the efficiency results for each model.

## Src Files

Overall, we can break the source code into the following categories:

1. Programs for to generate code

`closed_source_model_completion.py`: This program generates code outputs for closed source models such as GPT-4, GPT-4o-mini, etc.

`closed_source_model_plus_rag_completion.py`: This program generates code outputs for closed source models using a custom RAG pipeline.

`open_source_model_completion.py`: This program generates code outputs for open source models such as Llama. This script was unused and un-modified for the purpose of my experiments.

2. Programs to generate test cases

`gpt_generate_test_case_generator.py`: This script generates the file `./data/leetcode_with_test_generator.json`, which contains test cases for each LeetCode problem.

3. Programs for calculating efficiency

`code_efficiency_calculator.py`: This script generates efficiency metrics for a given model after its completions have been generated from any of the code generation scripts.

`generate_canonical_solutions.py`: This script generates efficiency metrics for the canonical solutions of each LeetCode problem.

4. Programs for generating reports

`report_overhead.py`: This script generates a report on the overhead of each model. The metrics output from this script are used to benchmark against other models and can be found in `./RESULTS.md`.

## Running Experiments

1. Install the required dependencies and move to the correct directory.

```bash
pip3 install -r requirements.txt
cd src
```

2. Generate code for a particular model using a script such as `closed_source_model_completion.py` or `closed_source_model_plus_rag_completion.py`.

```bash
python3 closed_source_model_completion.py --model gpt-4o-mini --embeddings_adapter adapter_10_lr0.01_no_negatives
```

Here, the `--model` flag specifies the model to use, and the `--embeddings_adapter` flag specifies the adapter as found in the `<ROOT>/model/adapters` directory to use.

3. Calculate the efficiency of the generated code using `code_efficiency_calculator.py`.

```bash
python3 code_efficiency_calculator.py --models gpt-4o-mini
```

Here, the `--models` flag specifies the model to use, which will correspond to the file-name in `./src/results/<file-name>.json`. Oftentimes, file-name will just be the name of the model; however, in special cases, such as when using a custom RAG pipeline, the file-name will be different.

4. Generate a report on the overhead of each model using `report_overhead.py`.

```bash
python3 report_overhead.py --models gpt-4o-mini
```

Again, the `--models` flag specifies the model to use. This time though, the file-name will be the same as the folder-name in `./src/dat_results/<folder-name>`.
