{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_config\n",
    "\n",
    "config = load_config(\"../config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The embeddings' evaluation will be used to measure the effectiveness of our trained embeddings models in a RAG pipeline.\n",
    "\n",
    "The metrics we'll use are as follows:\n",
    "\n",
    "1. Hit Rate @k:\n",
    "\n",
    "    - Measures whether the correct solution code snippet (ground truth) appear within the top k reults. This metrics is a binary metric for each query.\n",
    "    - Purpose: to measure the effectiveness of the embeddings in retrieving the correct code snippet.\n",
    "\n",
    "2. Mean Reciprocal Rank (MRR):\n",
    "    - Measures the average of the reciprocal ranks of the first correct solution code snippet (ground truth) in the top k results. This metric is a continuous metric for each query.\n",
    "    - Purpose: to measure the effectiveness of the embeddings in retrieving the correct code snippet.\n",
    "3. Mean Average Precision (MAP):\n",
    "    - Measures the average of the precision values at each relevant code snippet in the top k results. This metric is a continuous metric for each query.\n",
    "    - Purpose: to measure the effectiveness of the embeddings in retrieving the correct code snippet.\n",
    "4. Normalized Discounted Cumulative Gain (NDCG):\n",
    "    - Measures the ranking quality of the top k results. This metric is a continuous metric for each query.\n",
    "    - Purpose: to measure the effectiveness of the embeddings in retrieving the correct code snippet.\n",
    "\n",
    "The evaluation workflow is as follows:\n",
    "\n",
    "1. Load our corpus data, which is a collection of performant code snippets. These corpus data snippets are part of problem-code pairs, where the code is the solution to the problem. We expect to query the corpus by providing a problem statement and retrieve the most relevant code snippets.\n",
    "2. Load our embeddings models for testing; we will use the `all-MiniLM-L6-v2` model as a baseline.\n",
    "3. For each model, we will:\n",
    "    1. Generate embeddings for the problem statements.\n",
    "    2. Query the corpus using the problem statement embeddings.\n",
    "    3. Calculate the metrics for the retrieved code snippets.\n",
    "    4. Evaluate the retrieved code snippets against the ground truth code snippets, where we'd expect the retrieved code snippets to be the ground truth code snippets.\n",
    "4. Compare the performance of our trained models with the baseline model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import logging\n",
    "\n",
    "\n",
    "def setup_chromadb(path=\"chromadb\"):\n",
    "    logging.getLogger(\"chromadb\").setLevel(logging.WARNING)\n",
    "\n",
    "    client = chromadb.PersistentClient(path=path)\n",
    "    try:\n",
    "        client.delete_collection(\"paper_collection\")\n",
    "    except Exception as e:\n",
    "        print(f\"Collection deletion error: {e}\")\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        \"paper_collection\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Model Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def add_chunks_to_collection(collection, embedding_corpus):\n",
    "    def add_chunk(chunk, index):\n",
    "        collection.add(documents=[chunk], ids=[f\"chunk_{index}\"])\n",
    "        if index % 250 == 0:\n",
    "            print(f\"Added chunk {index}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        futures = [executor.submit(add_chunk, chunk['solution'], i)\n",
    "                   for i, chunk in enumerate(embedding_corpus)]\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def hit_rate(retrieved_docs, ground_truth, k):\n",
    "    return 1.0 if ground_truth in retrieved_docs[:k] else 0.0\n",
    "\n",
    "\n",
    "def reciprocal_rank(retrieved_docs, ground_truth, k):\n",
    "    try:\n",
    "        rank = retrieved_docs.index(ground_truth) + 1\n",
    "        return 1.0 / rank if rank <= k else 0.0\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def average_precision(retrieved_docs, ground_truth, k):\n",
    "    relevant_indices = [\n",
    "        i + 1 for i, doc in enumerate(retrieved_docs[:k]) if doc == ground_truth]\n",
    "    if not relevant_indices:\n",
    "        return 0.0\n",
    "    return np.mean([len(relevant_indices[:i + 1]) / rank for i, rank in enumerate(relevant_indices)])\n",
    "\n",
    "\n",
    "def mean_average_precision(retrieved_docs_list, ground_truth_list, k):\n",
    "    ap_scores = [average_precision(retrieved_docs, ground_truth, k)\n",
    "                 for retrieved_docs, ground_truth in zip(retrieved_docs_list, ground_truth_list)]\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "\n",
    "def discounted_cumulative_gain(retrieved_docs, ground_truth, k):\n",
    "    return sum(\n",
    "        (1 if doc == ground_truth else 0) / math.log2(rank + 1)\n",
    "        for rank, doc in enumerate(retrieved_docs[:k], start=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def normalized_discounted_cumulative_gain(retrieved_docs, ground_truth, k):\n",
    "    dcg = discounted_cumulative_gain(retrieved_docs, ground_truth, k)\n",
    "    idcg = sum(1 / math.log2(rank + 1) for rank in range(1, min(k, 1) + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_embeddings(collection, query_embedding, k=10):\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=k\n",
    "    )\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_query(query, base_model, adapter):\n",
    "    device = next(adapter.parameters()).device\n",
    "    query_emb = base_model.encode(query, convert_to_tensor=True).to(device)\n",
    "    adapted_query_emb = adapter(query_emb)\n",
    "    return adapted_query_emb.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def evaluate_model(validation_data, base_model, collection, k=100, adapter=None):\n",
    "    \"\"\"\n",
    "    Evaluates a given model on multiple metrics: Hit Rate, MRR, MAP, and NDCG.\n",
    "\n",
    "    Parameters:\n",
    "        validation_data (list): List of dictionaries with 'problem' and 'solution'.\n",
    "        base_model: The base embedding model used for encoding.\n",
    "        collection: The ChromaDB collection for document retrieval.\n",
    "        k (int): Number of top results to consider.\n",
    "        adapter (optional): An optional adapter model to apply to the query embeddings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with average values for Hit Rate, MRR, MAP, and NDCG.\n",
    "    \"\"\"\n",
    "    hit_rates = []\n",
    "    reciprocal_ranks = []\n",
    "    average_precisions = []\n",
    "    ndcgs = []\n",
    "\n",
    "    for data_point in validation_data:\n",
    "        question = data_point['problem']\n",
    "        ground_truth = data_point['solution']\n",
    "        question_embedding = base_model.encode(\n",
    "            question, convert_to_tensor=True)\n",
    "        if adapter is not None:\n",
    "            device = next(adapter.parameters()).device\n",
    "            question_embedding = adapter(\n",
    "                question_embedding.to(device)).cpu().detach().numpy()\n",
    "        else:\n",
    "            question_embedding = question_embedding.numpy()\n",
    "\n",
    "        retrieved_docs = retrieve_documents_embeddings(\n",
    "            collection, question_embedding, k)\n",
    "\n",
    "        hit_rates.append(hit_rate(retrieved_docs, ground_truth, k))\n",
    "        reciprocal_ranks.append(reciprocal_rank(\n",
    "            retrieved_docs, ground_truth, k))\n",
    "        average_precisions.append(average_precision(\n",
    "            retrieved_docs, ground_truth, k))\n",
    "        ndcgs.append(normalized_discounted_cumulative_gain(\n",
    "            retrieved_docs, ground_truth, k))\n",
    "\n",
    "    return {\n",
    "        'average_hit_rate': np.mean(hit_rates),\n",
    "        'average_reciprocal_rank': np.mean(reciprocal_ranks),\n",
    "        'mean_average_precision': np.mean(average_precisions),\n",
    "        'average_ndcg': np.mean(ndcgs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up ChromaDB collection...\n",
      "Loading and processing training data...\n",
      "Adding data chunks to ChromaDB collection...\n",
      "Added chunk 0\n",
      "Added chunk 250\n",
      "Added chunk 500\n",
      "Added chunk 750\n",
      "Added chunk 1000\n",
      "Added chunk 1250\n",
      "Added chunk 1500\n",
      "Added chunk 1750\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import load_data, LinearAdapter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Setting up ChromaDB collection...\")\n",
    "chromadb_collection = setup_chromadb()\n",
    "\n",
    "print(\"Loading and processing training data...\")\n",
    "training_data = load_data('../' + config.get('training_data_path'))\n",
    "train_data, val_data = train_test_split(\n",
    "    training_data, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Adding data chunks to ChromaDB collection...\")\n",
    "add_chunks_to_collection(chromadb_collection, train_data + val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Evaluating base model without adapter...\n",
      "Base Model - Average Hit Rate @1: 0.6164623467600701\n",
      "Base Model - Mean Reciprocal Rank @1: 0.6164623467600701\n",
      "Base Model - Mean Average Precision @1: 0.6164623467600701\n",
      "Base Model - Average NDCG @1: 0.6164623467600701\n",
      "\n",
      "Loading adapter model 1 from path: adapters/adapter_5_no_negatives.pth...\n",
      "Evaluating model with adapter 1...\n",
      "Adapter Model 1 - Average Hit Rate @1: 0.637478108581436\n",
      "Adapter Model 1 - Mean Reciprocal Rank @1: 0.637478108581436\n",
      "Adapter Model 1 - Mean Average Precision @1: 0.637478108581436\n",
      "Adapter Model 1 - Average NDCG @1: 0.637478108581436\n",
      "\n",
      "Loading adapter model 2 from path: adapters/adapter_5_negatives.pth...\n",
      "Evaluating model with adapter 2...\n",
      "Adapter Model 2 - Average Hit Rate @1: 0.03502626970227671\n",
      "Adapter Model 2 - Mean Reciprocal Rank @1: 0.03502626970227671\n",
      "Adapter Model 2 - Mean Average Precision @1: 0.03502626970227671\n",
      "Adapter Model 2 - Average NDCG @1: 0.03502626970227671\n",
      "Total execution time: 442.96 seconds\n"
     ]
    }
   ],
   "source": [
    "NUM_K = 1\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Evaluating base model without adapter...\")\n",
    "base_results = evaluate_model(\n",
    "    val_data, base_model, chromadb_collection, k=NUM_K)\n",
    "print(f\"Base Model - Average Hit Rate @{NUM_K}:\",\n",
    "      base_results['average_hit_rate'])\n",
    "print(f\"Base Model - Mean Reciprocal Rank @{NUM_K}:\",\n",
    "      base_results['average_reciprocal_rank'])\n",
    "print(f\"Base Model - Mean Average Precision @{NUM_K}:\",\n",
    "      base_results['mean_average_precision'])\n",
    "print(f\"Base Model - Average NDCG @{NUM_K}:\", base_results['average_ndcg'])\n",
    "\n",
    "adapters_to_eval_paths = config.get('adapters_to_eval_paths')\n",
    "\n",
    "for i, adapter_path in enumerate(adapters_to_eval_paths, start=1):\n",
    "    print(f\"\\nLoading adapter model {i} from path: {adapter_path}...\")\n",
    "    adapter = LinearAdapter(base_model.get_sentence_embedding_dimension())\n",
    "    adapter.load_state_dict(torch.load('../' + adapter_path)['adapter'])\n",
    "\n",
    "    print(f\"Evaluating model with adapter {i}...\")\n",
    "    adapter_results = evaluate_model(\n",
    "        val_data, base_model, chromadb_collection, k=NUM_K, adapter=adapter)\n",
    "\n",
    "    print(f\"Adapter Model {i} - Average Hit Rate @{NUM_K}:\",\n",
    "          adapter_results['average_hit_rate'])\n",
    "    print(f\"Adapter Model {i} - Mean Reciprocal Rank @{NUM_K}:\",\n",
    "          adapter_results['average_reciprocal_rank'])\n",
    "    print(f\"Adapter Model {i} - Mean Average Precision @{NUM_K}:\",\n",
    "          adapter_results['mean_average_precision'])\n",
    "    print(f\"Adapter Model {i} - Average NDCG @{NUM_K}:\",\n",
    "          adapter_results['average_ndcg'])\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total execution time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
